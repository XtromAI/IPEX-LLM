# Project: Lunar-LLM Galaxy Book5 Pro

## üéØ Project Goal
The objective of this repository is to build a high-performance, local inference environment specifically optimized for the **Intel Lunar Lake (Series 2)** architecture. 

By leveraging the **32GB of unified LPDDR5X memory** and the **Intel Arc 140V iGPU** on the Galaxy Book5 Pro, this project aims to run state-of-the-art models‚Äîincluding **Phi-4** and **Gemma 3 (27B)**‚Äîat usable speeds (5-15+ tokens/sec) without relying on cloud APIs.

---

## üß† Core Logic & Architecture

### 1. Hardware-Software Synergy (XPU Strategy)
Most local LLM tools default to NVIDIA (CUDA) or CPU. Our logic bypasses generic execution by using **IPEX-LLM (Intel Extension for PyTorch)**.
* **Target Device:** The code explicitly targets the `xpu` (Cross-Processor Unit), which allows the Arc iGPU to handle the heavy tensor math usually reserved for a dGPU.
* **Unified Memory Utilization:** Since the 32GB RAM is shared, we prioritize **INT4 and FP8 quantization**. This allows a 32B parameter model (which usually requires 64GB+ of VRAM) to fit into the system's LPDDR5X footprint.

### 2. The IPEX-LLM / SYCL Backend
The technical "brain" of the repo relies on the **oneAPI / SYCL** abstraction layer.
* **Kernel Compilation:** On the first run, the scripts trigger a JIT (Just-In-Time) compilation. This optimizes the model's kernels specifically for the Arc 140V‚Äôs execution units.
* **KV Cache Optimization:** We implement Intel-specific memory management to ensure that long-context conversations (up to 131k tokens) don't crash the system as the context window fills.

### 3. Execution Logic
1.  **Environment Isolation:** Use a specialized Conda environment to prevent conflict with standard PyTorch.
2.  **Environment Variable Injection:** Force hardware acceleration via `OLLAMA_NUM_GPU=999` and `ANY_ONEAPI_DEVICE_SELECTOR`.
3.  **Low-Bit Precision:** Use the `ipex-llm.transformers` library to load models in 4-bit precision during the initialization phase to minimize latency and heat.

---

## üõ†Ô∏è Implementation Pillars
* **Ollama (Intel-Optimized):** For a streamlined CLI-based chat experience.
* **Python/IPEX Inference:** For programmatic control and specialized "Reasoning" model chains (e.g. Phi-4).
* **Hardware Monitoring:** Integrated check-scripts to verify the NPU/iGPU load via Intel's System Management interface.