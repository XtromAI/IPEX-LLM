# OpenVINO GenAI Setup Log - December 28, 2025

## Summary

Successfully migrated from IPEX-LLM (v2.2.0, 8 months old) to OpenVINO GenAI (v2025.4.1.0, 2 weeks old) for LLM inference on Intel Arc 140V iGPU.

## Motivation

- IPEX-LLM project appears abandoned (last release April 2025)
- Stuck on Ollama v0.9.3 while latest is v0.13.5
- Need actively maintained solution with latest Intel GPU optimizations

## Setup Process

### 1. Environment Creation

```powershell
conda create -n openvino-genai python=3.11 -y
```

**Result:** Environment created successfully at `C:\Users\creks\miniconda3\envs\openvino-genai`

### 2. Package Installation

```powershell
conda activate openvino-genai
pip install openvino-genai optimum-intel
pip install nncf  # Required for INT4 quantization
```

**Packages installed:**
- openvino-genai 2025.4.1.0-1901
- openvino 2025.4.1-20426
- openvino-tokenizers 2025.4.1.0
- optimum-intel 1.27.0
- optimum 2.1.0
- nncf 2.19.0
- torch 2.9.1
- transformers 4.57.3
- Plus dependencies...

**Total download size:** ~200 MB

### 3. Verification

```powershell
# Check version
python -c "import openvino_genai as ov_genai; print(f'OpenVINO GenAI version: {ov_genai.__version__}')"
# Output: OpenVINO GenAI version: 2025.4.1.0-2683-fc593653d77

# Check GPU detection
python -c "from openvino import Core; devices = Core().available_devices; print('Available devices:', devices); print('GPU detected:', 'GPU' in devices)"
# Output: Available devices: ['CPU', 'GPU', 'NPU']
# GPU detected: True
```

**Result:** ✅ All devices detected correctly including Intel Arc 140V GPU

### 4. Model Conversion

```powershell
optimum-cli export openvino --model TinyLlama/TinyLlama-1.1B-Chat-v1.0 --weight-format int4 --trust-remote-code TinyLlama-1.1B-ov
```

**Process:**
1. Downloaded model: 2.20 GB from HuggingFace (~1.5 minutes)
2. Applied data-free AWQ quantization (~15 seconds)
3. Applied weight compression to INT4 (~6 seconds)
4. Created OpenVINO IR format

**Output directory:** `C:\Users\creks\Documents\IPEX-LLM\TinyLlama-1.1B-ov`

**Final size:** ~1.3 GB (from 2.2 GB original)

**Quantization stats:**
- 88% of layers: INT4 asymmetric, group size 64
- 12% of layers: INT8 asymmetric, per-channel
- 100% of ratio-defining parameters in INT4

### 5. Inference Testing

**Test script:** `test_openvino.py`

```python
import openvino_genai as ov_genai
import time

model_path = "TinyLlama-1.1B-ov"
pipe = ov_genai.LLMPipeline(model_path, "GPU")

prompt = "Explain the benefits of unified memory architecture in 3 sentences."
start = time.time()
response = pipe.generate(prompt, max_new_tokens=256)
elapsed = time.time() - start
```

**Results:**
- Model load time: 7.35 seconds
- First inference (warm-up): ~0.5 seconds (5 tokens)
- Main generation: 0.95 seconds
- Tokens generated: 67 words
- Generation speed: **~70.5 words/second**

**Response quality:** Coherent and accurate explanation of unified memory architecture with proper formatting (numbered list).

### 6. Interactive Chat

**Chat script:** `chat_openvino.py`

Features implemented:
- Multi-turn conversation with history
- Commands: `exit`, `quit`, `clear`
- Context management (keeps last 6 exchanges)
- Clean response parsing

**Status:** ✅ Ready to use

## Performance Analysis

### TinyLlama-1.1B-Chat on Arc 140V

| Metric | Value |
|--------|-------|
| Model size | 1.3 GB (INT4) |
| Load time | 7.35s |
| First token latency | ~0.5s |
| Generation speed | 70.5 words/s |
| Memory usage | ~2 GB GPU |

### Comparison with IPEX-LLM

| Aspect | IPEX-LLM | OpenVINO GenAI |
|--------|----------|----------------|
| Last update | April 2025 (8 months ago) | December 2025 (2 weeks ago) |
| Version | 2.2.0 | 2025.4.1.0 |
| Ollama version | v0.9.3 | N/A (direct Python API) |
| Installation | Conda + manual symlinks | Simple pip install |
| Setup complexity | Complex (portable bundle) | Simple (standard packages) |
| Model format | GGUF | OpenVINO IR |
| GPU detection | Via env vars | Automatic |
| Update frequency | Inactive | Active (bi-weekly) |

## Files Created

### Documentation
- `docs/openvino-setup.md` - Complete setup guide and reference
- `docs/setup-log-2025-12-28.md` - This file

### Scripts
- `test_openvino.py` - Performance testing script
- `chat_openvino.py` - Interactive chat interface

### Models
- `TinyLlama-1.1B-ov/` - Converted model directory

## Known Issues

### HuggingFace Symlinks Warning

```
UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently 
store duplicated files but your machine does not support them
```

**Impact:** Non-critical, models will download but may use more disk space

**Fix:** Enable Windows Developer Mode or run as Administrator (not required for functionality)

### Xet Storage Messages

```
Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed.
```

**Impact:** None - falls back to regular HTTP download

**Fix:** Optional: `pip install huggingface_hub[hf_xet]`

## Environment Variables

No special environment variables needed for basic operation. OpenVINO automatically detects and uses the Intel Arc GPU via Level Zero.

For advanced optimization, can optionally set:
```powershell
$env:ZES_ENABLE_SYSMAN = "1"
$env:SYCL_PI_LEVEL_ZERO_USE_IMMEDIATE_COMMANDLISTS = "1"
$env:OV_CACHE_DIR = "$env:USERPROFILE\.cache\openvino"
```

## Next Steps

### Immediate (Priority 1)
1. ✅ Complete setup and verify working
2. ⏭️ Convert Llama 3.2 3B for direct comparison with IPEX-LLM
3. ⏭️ Benchmark against existing gemma3:12b results

### Short-term (Priority 2)
1. Convert medium models: Phi-3.5-mini, Mistral-7B
2. Build model management script
3. Create performance comparison dashboard

### Long-term (Priority 3)
1. Build REST API wrapper for Ollama-compatible interface
2. Test Visual Language Models (LLaVA, MiniCPM-V)
3. Explore quantization options (INT8, FP16)
4. Test NPU acceleration if supported

## Conclusion

OpenVINO GenAI setup successful and verified working on Intel Arc 140V. System is production-ready for LLM inference with:
- ✅ Automatic GPU detection
- ✅ Fast inference (70+ words/s on 1.1B model)
- ✅ Simple Python API
- ✅ Active development and support
- ✅ Easy model conversion from HuggingFace

Recommended to proceed with migration from IPEX-LLM due to superior maintenance and update frequency.
