# Quickstart: Core Inference Setup

## Prerequisites
- Windows 11
- Intel Arc 140V iGPU (Lunar Lake)
- Git installed

## 1. Install Ollama Portable

Run the automated setup script to download and extract Intel's Ollama Portable Zip.

```powershell
./scripts/setup-ollama-portable.ps1
```

This will:
1. Download the Intel-provided Ollama build.
2. Extract it into `ollama-portable/`.
3. Create convenience scripts under `scripts/` for starting the server and using the CLI.

## 2. Start the Ollama Server

```powershell
./scripts/start-ollama-server.ps1
```

Keep the server window running while you use models.

## 3. Verify Inference

In a new terminal:

```powershell
cd ollama-portable
./ollama.exe run llama3.2:3b
```

If successful, you should see a response generated by the model running on the Intel Arc 140V XPU.

