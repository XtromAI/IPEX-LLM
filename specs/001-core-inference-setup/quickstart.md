# Quickstart: Core Inference Setup

## Prerequisites
- Windows 11
- Intel Arc 140V iGPU (Lunar Lake)
- Miniconda or Anaconda installed
- Git installed

## 1. Environment Setup

Run the automated setup script to create the `ipex-llm` Conda environment and install dependencies.

```powershell
./scripts/setup-env.ps1
```

This will:
1. Create a Conda environment named `ipex-llm`.
2. Install Python 3.10.
3. Install `ipex-llm[xpu]`, `torch`, and other required packages.

## 2. Verify Installation

Activate the environment and run the verification script.

```powershell
conda activate ipex-llm
python scripts/run-inference.py --prompt "Hello, world!"
```

If successful, you should see output generated by the Llama 3.1 8B model running on the XPU.

## 3. Serve via Ollama

To start the Ollama server with Intel XPU optimization:

```powershell
./scripts/start-ollama.ps1
```

Then, in a separate terminal:

```powershell
ollama run llama3.1
```
